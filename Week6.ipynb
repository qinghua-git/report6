{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_events = pd.read_csv (r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\machine_events\\part-00000-of-00001.csv',header=None)\n",
    "machine_events.columns = [\"time\", \"machine ID\", \"event type\", \"platform ID\", \"CPUs\",\"Memory\"]\n",
    "machine_events = machine_events.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                time  machine ID  event type  \\\n",
      "0                  0           5           0   \n",
      "1                  0           6           0   \n",
      "2                  0           7           0   \n",
      "3                  0          10           0   \n",
      "4                  0          13           0   \n",
      "...              ...         ...         ...   \n",
      "37775  2505897195359     5781239           2   \n",
      "37776  2506070744967  3463632642           0   \n",
      "37777  2506097126581  2343531369           1   \n",
      "37778  2506129491172  4246147567           0   \n",
      "37779  2506135493517  4246147567           1   \n",
      "\n",
      "                                        platform ID  CPUs  Memory  \n",
      "0      HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=   0.5  0.2493  \n",
      "1      HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=   0.5  0.2493  \n",
      "2      HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=   0.5  0.2493  \n",
      "3      HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=   0.5  0.2493  \n",
      "4      HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=   0.5  0.2493  \n",
      "...                                             ...   ...     ...  \n",
      "37775  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=   0.5  0.2493  \n",
      "37776  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=   0.5  0.4995  \n",
      "37777  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=   0.5  0.2493  \n",
      "37778  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=   0.5  0.7490  \n",
      "37779  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=   0.5  0.7490  \n",
      "\n",
      "[37748 rows x 6 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "time           37748\n",
       "machine ID     37748\n",
       "event type     37748\n",
       "platform ID    37748\n",
       "CPUs           37748\n",
       "Memory         37748\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (machine_events)\n",
    "machine_events.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_table = machine_events[machine_events['event type'] == 0]\n",
    "machine_table = machine_table[['machine ID', 'platform ID', 'CPUs', 'Memory']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       machine ID                                   platform ID  CPUs  Memory\n",
      "0               5  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=  0.50  0.2493\n",
      "1               6  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=  0.50  0.2493\n",
      "2               7  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=  0.50  0.2493\n",
      "3              10  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=  0.50  0.2493\n",
      "4              13  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=  0.50  0.2493\n",
      "...           ...                                           ...   ...     ...\n",
      "37765  3938706287  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=  0.50  0.4995\n",
      "37769  3337972792  70ZOvysYGtB6j9MUHMPzA2Iy7GRzWeJTdX0YCLRKGVg=  0.25  0.2498\n",
      "37773  4478494145  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=  0.50  0.4995\n",
      "37776  3463632642  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=  0.50  0.4995\n",
      "37778  4246147567  HofLGzk1Or/8Ildj2+Lqv0UGGvY82NLoni8+J/Yy0RU=  0.50  0.7490\n",
      "\n",
      "[21411 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "machine ID     21411\n",
       "platform ID    21411\n",
       "CPUs           21411\n",
       "Memory         21411\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (machine_table)\n",
    "machine_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_table.to_csv(\"machine_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2011742\n",
       "1    2011742\n",
       "2    2011742\n",
       "3    2011742\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\job_events'\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [2, 3, 5, 6])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "job_events = pd.DataFrame(comb_np_array)\n",
    "job_events.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_events.columns = [\"job ID\", \"event type\", \"job scheduling class\", \"job name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             job ID event type job scheduling class  \\\n",
      "0           3418314          0                    3   \n",
      "1           3418319          0                    3   \n",
      "2           3418324          0                    3   \n",
      "3           3418329          0                    3   \n",
      "4           3418334          0                    3   \n",
      "...             ...        ...                  ...   \n",
      "2011737  6486630408          4                    0   \n",
      "2011738  6486630347          4                    2   \n",
      "2011739  6486630448          5                    2   \n",
      "2011740  6486641236          4                    0   \n",
      "2011741  6486611683          4                    1   \n",
      "\n",
      "                                             job name  \n",
      "0        L52XDyhi9x9ChmVBZ1qavOFmnzPeVsvQ2QyGmBZcV4s=  \n",
      "1        vq0IN3BWEbkDjYgYvkrVyH6OWoUoDwFFf3j/syEZzLA=  \n",
      "2        X+Vce15Yu3BCKb7Ttc6hvINAzdfG3NtYEDNNsPdMGKo=  \n",
      "3        EeK3DUWYi1P0vgBTp7wZdUos8UKj/+/FqudTLohMQ9M=  \n",
      "4        noCrQkR+8CU32ibuNmNvobHFFGuXRZ2aM/ZvcMHaOg4=  \n",
      "...                                               ...  \n",
      "2011737  xb3x54h7E+9rLNX5m7P3gBQW4LpAMV7paZrdAVTzHHY=  \n",
      "2011738  Bs0EtUaS5ZALUTq2wA8DdbdtxxOXSaVOIuBaCdieMT8=  \n",
      "2011739  LfrpKk2mrUwtwAaHBO9ZCfX45HrO61YnsjXrLmO83H8=  \n",
      "2011740  6cEyQ7YEIiUwugpf7HOg87W3QG7RZOrfzsyOwZwMszw=  \n",
      "2011741  olYhgoewCyUm+KD8M7Q3ihnXAfxHH9nnxg+f4vT0YEU=  \n",
      "\n",
      "[2011742 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print (job_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                  1343878\n",
       "event type              1343878\n",
       "job scheduling class    1343878\n",
       "job name                1343878\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options = [0, 1] \n",
    "job_events_type01 = job_events[job_events['event type'].isin(options)] \n",
    "job_events_type01.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             job ID event type job scheduling class  \\\n",
      "0           3418314          0                    3   \n",
      "1           3418319          0                    3   \n",
      "2           3418324          0                    3   \n",
      "3           3418329          0                    3   \n",
      "4           3418334          0                    3   \n",
      "...             ...        ...                  ...   \n",
      "2011724  6486638013          1                    1   \n",
      "2011725  6486638079          0                    1   \n",
      "2011726  6486638079          1                    1   \n",
      "2011728  6486641236          0                    0   \n",
      "2011730  6486641236          1                    0   \n",
      "\n",
      "                                             job name  \n",
      "0        L52XDyhi9x9ChmVBZ1qavOFmnzPeVsvQ2QyGmBZcV4s=  \n",
      "1        vq0IN3BWEbkDjYgYvkrVyH6OWoUoDwFFf3j/syEZzLA=  \n",
      "2        X+Vce15Yu3BCKb7Ttc6hvINAzdfG3NtYEDNNsPdMGKo=  \n",
      "3        EeK3DUWYi1P0vgBTp7wZdUos8UKj/+/FqudTLohMQ9M=  \n",
      "4        noCrQkR+8CU32ibuNmNvobHFFGuXRZ2aM/ZvcMHaOg4=  \n",
      "...                                               ...  \n",
      "2011724  hUWYVElFOTQFBOvH8c93V3Z7ASi29Mk9Yg4tc5PCTG4=  \n",
      "2011725  9XfxgqyEqcJEDRFyjlU9D0jaPwhgk3S02Eb/ZyCdD3M=  \n",
      "2011726  9XfxgqyEqcJEDRFyjlU9D0jaPwhgk3S02Eb/ZyCdD3M=  \n",
      "2011728  6cEyQ7YEIiUwugpf7HOg87W3QG7RZOrfzsyOwZwMszw=  \n",
      "2011730  6cEyQ7YEIiUwugpf7HOg87W3QG7RZOrfzsyOwZwMszw=  \n",
      "\n",
      "[1343878 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print (job_events_type01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_events_type01.to_csv(\"job_events_type01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14881528\n",
       "1    14881528\n",
       "2    14881528\n",
       "3    14881528\n",
       "4    14881238\n",
       "5    14881238\n",
       "6    14881238\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_events\\1\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [2,3,5,7,9,10,11])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_events1 = pd.DataFrame(comb_np_array)\n",
    "task_events1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               job ID  task index  event type  task scheduling class  \\\n",
      "0           3418309.0         1.0         0.0                    3.0   \n",
      "1           3418314.0         0.0         0.0                    3.0   \n",
      "2           3418314.0         1.0         0.0                    3.0   \n",
      "3           3418319.0         0.0         0.0                    3.0   \n",
      "4           3418319.0         1.0         0.0                    3.0   \n",
      "...               ...         ...         ...                    ...   \n",
      "14881523  515042969.0        14.0         0.0                    2.0   \n",
      "14881524  515042969.0        18.0         5.0                    2.0   \n",
      "14881525  515042969.0        18.0         0.0                    2.0   \n",
      "14881526  515042969.0        24.0         5.0                    2.0   \n",
      "14881527  515042969.0        24.0         0.0                    2.0   \n",
      "\n",
      "          task CPU request  task memory request  task disk space request  \n",
      "0                      NaN                  NaN                      NaN  \n",
      "1                  0.12500              0.07446                 0.000424  \n",
      "2                  0.12500              0.07446                 0.000424  \n",
      "3                      NaN                  NaN                      NaN  \n",
      "4                      NaN                  NaN                      NaN  \n",
      "...                    ...                  ...                      ...  \n",
      "14881523           0.01562              0.01553                 0.000215  \n",
      "14881524           0.01562              0.01553                 0.000215  \n",
      "14881525           0.01562              0.01553                 0.000215  \n",
      "14881526           0.01562              0.01553                 0.000215  \n",
      "14881527           0.01562              0.01553                 0.000215  \n",
      "\n",
      "[14881528 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "task_events1.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request'] \n",
    "print (task_events1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events_1 = task_events1.dropna(subset=['task CPU request', 'task memory request', 'task disk space request'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                job ID  task index  event type  task scheduling class  \\\n",
      "1         3.418314e+06         0.0         0.0                    3.0   \n",
      "2         3.418314e+06         1.0         0.0                    3.0   \n",
      "44        3.418368e+06         0.0         0.0                    3.0   \n",
      "45        3.418368e+06         1.0         0.0                    3.0   \n",
      "46        3.418368e+06         2.0         0.0                    3.0   \n",
      "...                ...         ...         ...                    ...   \n",
      "14881502  6.146435e+09        37.0         1.0                    3.0   \n",
      "14881507  6.266780e+09         0.0         0.0                    2.0   \n",
      "14881523  5.150430e+08        14.0         0.0                    2.0   \n",
      "14881525  5.150430e+08        18.0         0.0                    2.0   \n",
      "14881527  5.150430e+08        24.0         0.0                    2.0   \n",
      "\n",
      "          task CPU request  task memory request  task disk space request  \n",
      "1                 0.125000              0.07446                 0.000424  \n",
      "2                 0.125000              0.07446                 0.000424  \n",
      "44                0.031250              0.08691                 0.000455  \n",
      "45                0.031250              0.08691                 0.000455  \n",
      "46                0.031250              0.08691                 0.000455  \n",
      "...                    ...                  ...                      ...  \n",
      "14881502          0.062500              0.11740                 0.000623  \n",
      "14881507          0.006248              0.03107                 0.000038  \n",
      "14881523          0.015620              0.01553                 0.000215  \n",
      "14881525          0.015620              0.01553                 0.000215  \n",
      "14881527          0.015620              0.01553                 0.000215  \n",
      "\n",
      "[9977438 rows x 7 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "job ID                     9977438\n",
       "task index                 9977438\n",
       "event type                 9977438\n",
       "task scheduling class      9977438\n",
       "task CPU request           9977438\n",
       "task memory request        9977438\n",
       "task disk space request    9977438\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options = [0, 1] \n",
    "task_events_1 = task_events_1[task_events_1['event type'].isin(options)] \n",
    "#task_events_1 = task_events_1[['job ID', 'task index', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']]\n",
    "print (task_events_1)\n",
    "task_events_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events_1.to_csv(\"task_events_1.csv\", index = False)\n",
    "task_events_1.to_csv(\"task_events_1modified.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     8931839\n",
       "task index                 8931839\n",
       "event type                 8931839\n",
       "task scheduling class      8931839\n",
       "task CPU request           8931839\n",
       "task memory request        8931839\n",
       "task disk space request    8931839\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_events\\2\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [2,3,5,7,9,10,11])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_events2 = pd.DataFrame(comb_np_array)\n",
    "task_events2.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']\n",
    "task_events2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     5932953\n",
       "task index                 5932953\n",
       "event type                 5932953\n",
       "task scheduling class      5932953\n",
       "task CPU request           5932953\n",
       "task memory request        5932953\n",
       "task disk space request    5932953\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_events_2 = task_events2.dropna(subset=['task CPU request', 'task memory request', 'task disk space request'])\n",
    "options = [0, 1] \n",
    "task_events_2 = task_events_2[task_events_2['event type'].isin(options)] \n",
    "#task_events_2 = task_events_2[['job ID', 'task index', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']]\n",
    "task_events_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events_2.to_csv(\"task_events_2.csv\", index = False)\n",
    "task_events_2.to_csv(\"task_events_2modified.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     9145850\n",
       "task index                 9145850\n",
       "event type                 9145850\n",
       "task scheduling class      9145850\n",
       "task CPU request           9145850\n",
       "task memory request        9145850\n",
       "task disk space request    9145850\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_events\\3\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [2,3,5,7,9,10,11])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_events3 = pd.DataFrame(comb_np_array)\n",
    "task_events3.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']\n",
    "task_events3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     6046874\n",
       "task index                 6046874\n",
       "event type                 6046874\n",
       "task scheduling class      6046874\n",
       "task CPU request           6046874\n",
       "task memory request        6046874\n",
       "task disk space request    6046874\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_events_3 = task_events3.dropna(subset=['task CPU request', 'task memory request', 'task disk space request'])\n",
    "options = [0, 1] \n",
    "task_events_3 = task_events_3[task_events_3['event type'].isin(options)] \n",
    "#task_events_3 = task_events_3[['job ID', 'task index', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']]\n",
    "task_events_3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events_3.to_csv(\"task_events_3.csv\", index = False)\n",
    "task_events_3.to_csv(\"task_events_3modified.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     19795789\n",
       "task index                 19795789\n",
       "event type                 19795789\n",
       "task scheduling class      19795789\n",
       "task CPU request           19795789\n",
       "task memory request        19795789\n",
       "task disk space request    19795789\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_events\\4\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [2,3,5,7,9,10,11])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_events4 = pd.DataFrame(comb_np_array)\n",
    "task_events4.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']\n",
    "task_events4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     13146463\n",
       "task index                 13146463\n",
       "event type                 13146463\n",
       "task scheduling class      13146463\n",
       "task CPU request           13146463\n",
       "task memory request        13146463\n",
       "task disk space request    13146463\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_events_4 = task_events4.dropna(subset=['task CPU request', 'task memory request', 'task disk space request'])\n",
    "options = [0, 1] \n",
    "task_events_4 = task_events_4[task_events_4['event type'].isin(options)] \n",
    "#task_events_4 = task_events_4[['job ID', 'task index', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']]\n",
    "task_events_4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events_4.to_csv(\"task_events_4.csv\", index = False)\n",
    "task_events_4.to_csv(\"task_events_4modified.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     17664706\n",
       "task index                 17664706\n",
       "event type                 17664706\n",
       "task scheduling class      17664706\n",
       "task CPU request           17664706\n",
       "task memory request        17664706\n",
       "task disk space request    17664706\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_events\\5\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [2,3,5,7,9,10,11])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_events5 = pd.DataFrame(comb_np_array)\n",
    "task_events5.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']\n",
    "task_events5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     11697393\n",
       "task index                 11697393\n",
       "event type                 11697393\n",
       "task scheduling class      11697393\n",
       "task CPU request           11697393\n",
       "task memory request        11697393\n",
       "task disk space request    11697393\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_events_5 = task_events5.dropna(subset=['task CPU request', 'task memory request', 'task disk space request'])\n",
    "options = [0, 1] \n",
    "task_events_5 = task_events_5[task_events_5['event type'].isin(options)] \n",
    "#task_events_5 = task_events_5[['job ID', 'task index', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']]\n",
    "task_events_5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events_5.to_csv(\"task_events_5.csv\", index = False)\n",
    "task_events_5.to_csv(\"task_events_5modified.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     13923370\n",
       "task index                 13923370\n",
       "event type                 13923370\n",
       "task scheduling class      13923370\n",
       "task CPU request           13923370\n",
       "task memory request        13923370\n",
       "task disk space request    13923370\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_events\\6\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [2,3,5,7,9,10,11])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_events6 = pd.DataFrame(comb_np_array)\n",
    "task_events6.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']\n",
    "task_events6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     9269690\n",
       "task index                 9269690\n",
       "event type                 9269690\n",
       "task scheduling class      9269690\n",
       "task CPU request           9269690\n",
       "task memory request        9269690\n",
       "task disk space request    9269690\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_events_6 = task_events6.dropna(subset=['task CPU request', 'task memory request', 'task disk space request'])\n",
    "options = [0, 1] \n",
    "task_events_6 = task_events_6[task_events_6['event type'].isin(options)] \n",
    "#task_events_6 = task_events_6[['job ID', 'task index', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']]\n",
    "task_events_6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events_6.to_csv(\"task_events_6.csv\", index = False)\n",
    "task_events_6.to_csv(\"task_events_6modified.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     15172043\n",
       "task index                 15172043\n",
       "event type                 15172043\n",
       "task scheduling class      15172043\n",
       "task CPU request           15172043\n",
       "task memory request        15172043\n",
       "task disk space request    15172043\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_events\\7\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [2,3,5,7,9,10,11])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_events7 = pd.DataFrame(comb_np_array)\n",
    "task_events7.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']\n",
    "task_events7.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     10025994\n",
       "task index                 10025994\n",
       "event type                 10025994\n",
       "task scheduling class      10025994\n",
       "task CPU request           10025994\n",
       "task memory request        10025994\n",
       "task disk space request    10025994\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_events_7 = task_events7.dropna(subset=['task CPU request', 'task memory request', 'task disk space request'])\n",
    "options = [0, 1] \n",
    "task_events_7 = task_events_7[task_events_7['event type'].isin(options)] \n",
    "#task_events_7 = task_events_7[['job ID', 'task index', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']]\n",
    "task_events_7.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events_7.to_csv(\"task_events_7.csv\", index = False)\n",
    "task_events_7.to_csv(\"task_events_7modified.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     11237143\n",
       "task index                 11237143\n",
       "event type                 11237143\n",
       "task scheduling class      11237143\n",
       "task CPU request           11237143\n",
       "task memory request        11237143\n",
       "task disk space request    11237143\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_events\\8\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [2,3,5,7,9,10,11])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_events8 = pd.DataFrame(comb_np_array)\n",
    "task_events8.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']\n",
    "task_events8.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     7453121\n",
       "task index                 7453121\n",
       "event type                 7453121\n",
       "task scheduling class      7453121\n",
       "task CPU request           7453121\n",
       "task memory request        7453121\n",
       "task disk space request    7453121\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_events_8 = task_events8.dropna(subset=['task CPU request', 'task memory request', 'task disk space request'])\n",
    "options = [0, 1] \n",
    "task_events_8 = task_events_8[task_events_8['event type'].isin(options)] \n",
    "#task_events_8 = task_events_8[['job ID', 'task index', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']]\n",
    "task_events_8.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events_8.to_csv(\"task_events_8.csv\", index = False)\n",
    "task_events_8.to_csv(\"task_events_8modified.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     10834405\n",
       "task index                 10834405\n",
       "event type                 10834405\n",
       "task scheduling class      10834405\n",
       "task CPU request           10834405\n",
       "task memory request        10834405\n",
       "task disk space request    10834405\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_events\\9\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [2,3,5,7,9,10,11])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_events9 = pd.DataFrame(comb_np_array)\n",
    "task_events9.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']\n",
    "task_events9.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     7142576\n",
       "task index                 7142576\n",
       "event type                 7142576\n",
       "task scheduling class      7142576\n",
       "task CPU request           7142576\n",
       "task memory request        7142576\n",
       "task disk space request    7142576\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_events_9 = task_events9.dropna(subset=['task CPU request', 'task memory request', 'task disk space request'])\n",
    "options = [0, 1] \n",
    "task_events_9 = task_events_9[task_events_9['event type'].isin(options)] \n",
    "#task_events_9 = task_events_9[['job ID', 'task index', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']]\n",
    "task_events_9.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events_9.to_csv(\"task_events_9.csv\", index = False)\n",
    "task_events_9.to_csv(\"task_events_9modified.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     12243045\n",
       "task index                 12243045\n",
       "event type                 12243045\n",
       "task scheduling class      12243045\n",
       "task CPU request           12243045\n",
       "task memory request        12243045\n",
       "task disk space request    12243045\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_events\\10\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [2,3,5,7,9,10,11])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_events10 = pd.DataFrame(comb_np_array)\n",
    "task_events10.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']\n",
    "task_events10.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     8069386\n",
       "task index                 8069386\n",
       "event type                 8069386\n",
       "task scheduling class      8069386\n",
       "task CPU request           8069386\n",
       "task memory request        8069386\n",
       "task disk space request    8069386\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_events_10 = task_events10.dropna(subset=['task CPU request', 'task memory request', 'task disk space request'])\n",
    "options = [0, 1] \n",
    "task_events_10 = task_events_10[task_events_10['event type'].isin(options)] \n",
    "#task_events_10 = task_events_10[['job ID', 'task index', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']]\n",
    "task_events_10.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events_10.to_csv(\"task_events_10.csv\", index = False)\n",
    "task_events_10.to_csv(\"task_events_10modified.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     10818070\n",
       "task index                 10818070\n",
       "event type                 10818070\n",
       "task scheduling class      10818070\n",
       "task CPU request           10818070\n",
       "task memory request        10818070\n",
       "task disk space request    10818070\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_events\\11\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [2,3,5,7,9,10,11])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_events11 = pd.DataFrame(comb_np_array)\n",
    "task_events11.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']\n",
    "task_events11.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job ID                     6963799\n",
       "task index                 6963799\n",
       "event type                 6963799\n",
       "task scheduling class      6963799\n",
       "task CPU request           6963799\n",
       "task memory request        6963799\n",
       "task disk space request    6963799\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_events_11 = task_events11.dropna(subset=['task CPU request', 'task memory request', 'task disk space request'])\n",
    "options = [0, 1] \n",
    "task_events_11 = task_events_11[task_events_11['event type'].isin(options)] \n",
    "#task_events_11 = task_events_11[['job ID', 'task index', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request']]\n",
    "task_events_11.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events_11.to_csv(\"task_events_11.csv\", index = False)\n",
    "task_events_11.to_csv(\"task_events_11modified.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start time                12210229\n",
       "end time                  12210229\n",
       "job ID                    12210229\n",
       "task index                12210229\n",
       "CPU rate                  12210229\n",
       "canonical memory usage    12210229\n",
       "assigned memory usage     12210229\n",
       "dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\1\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage1 = pd.DataFrame(comb_np_array)\n",
    "task_usage1.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start time    12210229\n",
       "end time      12210229\n",
       "job ID        12210229\n",
       "task index    12210229\n",
       "dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_usage_1 = task_usage1.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_1 = task_usage_1[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_usage_1.to_csv(\"task_usage_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time                12942126\n",
      "end time                  12942126\n",
      "job ID                    12942126\n",
      "task index                12942126\n",
      "CPU rate                  12942126\n",
      "canonical memory usage    12942126\n",
      "assigned memory usage     12942126\n",
      "dtype: int64\n",
      "start time    12942126\n",
      "end time      12942126\n",
      "job ID        12942126\n",
      "task index    12942126\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\2\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage2 = pd.DataFrame(comb_np_array)\n",
    "task_usage2.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "print (task_usage2.count())\n",
    "task_usage_2 = task_usage2.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_2 = task_usage_2[['start time', 'end time', 'job ID', 'task index']]\n",
    "print (task_usage_2.count())\n",
    "task_usage_2.to_csv(\"task_usage_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time                12892177\n",
      "end time                  12892177\n",
      "job ID                    12892177\n",
      "task index                12892177\n",
      "CPU rate                  12892177\n",
      "canonical memory usage    12892177\n",
      "assigned memory usage     12892177\n",
      "dtype: int64\n",
      "start time    12892177\n",
      "end time      12892177\n",
      "job ID        12892177\n",
      "task index    12892177\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\3\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage3 = pd.DataFrame(comb_np_array)\n",
    "task_usage3.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "print (task_usage3.count())\n",
    "task_usage_3 = task_usage3.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_3 = task_usage_3[['start time', 'end time', 'job ID', 'task index']]\n",
    "print (task_usage_3.count())\n",
    "task_usage_3.to_csv(\"task_usage_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time                16337285\n",
      "end time                  16337285\n",
      "job ID                    16337285\n",
      "task index                16337285\n",
      "CPU rate                  16337285\n",
      "canonical memory usage    16337285\n",
      "assigned memory usage     16337285\n",
      "dtype: int64\n",
      "start time    16337285\n",
      "end time      16337285\n",
      "job ID        16337285\n",
      "task index    16337285\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\4\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage4 = pd.DataFrame(comb_np_array)\n",
    "task_usage4.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "print (task_usage4.count())\n",
    "task_usage_4 = task_usage4.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_4 = task_usage_4[['start time', 'end time', 'job ID', 'task index']]\n",
    "print (task_usage_4.count())\n",
    "task_usage_4.to_csv(\"task_usage_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time                19508206\n",
      "end time                  19508206\n",
      "job ID                    19508206\n",
      "task index                19508206\n",
      "CPU rate                  19508206\n",
      "canonical memory usage    19508206\n",
      "assigned memory usage     19508206\n",
      "dtype: int64\n",
      "start time    19508206\n",
      "end time      19508206\n",
      "job ID        19508206\n",
      "task index    19508206\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\5\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage5 = pd.DataFrame(comb_np_array)\n",
    "task_usage5.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "print (task_usage5.count())\n",
    "task_usage_5 = task_usage5.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_5 = task_usage_5[['start time', 'end time', 'job ID', 'task index']]\n",
    "print (task_usage_5.count())\n",
    "task_usage_5.to_csv(\"task_usage_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time                12940708\n",
      "end time                  12940708\n",
      "job ID                    12940708\n",
      "task index                12940708\n",
      "CPU rate                  12940708\n",
      "canonical memory usage    12940708\n",
      "assigned memory usage     12940708\n",
      "dtype: int64\n",
      "start time    12940708\n",
      "end time      12940708\n",
      "job ID        12940708\n",
      "task index    12940708\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\6\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage6 = pd.DataFrame(comb_np_array)\n",
    "task_usage6.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "print (task_usage6.count())\n",
    "task_usage_6 = task_usage6.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_6 = task_usage_6[['start time', 'end time', 'job ID', 'task index']]\n",
    "print (task_usage_6.count())\n",
    "task_usage_6.to_csv(\"task_usage_6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\7\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage7 = pd.DataFrame(comb_np_array)\n",
    "task_usage7.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_7 = task_usage7.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_7 = task_usage_7[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_7.to_csv(\"task_usage_7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\8\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage8 = pd.DataFrame(comb_np_array)\n",
    "task_usage8.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_8 = task_usage8.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_8 = task_usage_8[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_8.to_csv(\"task_usage_8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\9\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage9 = pd.DataFrame(comb_np_array)\n",
    "task_usage9.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_9 = task_usage9.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_9 = task_usage_9[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_9.to_csv(\"task_usage_9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\10\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage10 = pd.DataFrame(comb_np_array)\n",
    "task_usage10.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_10 = task_usage10.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_10 = task_usage_10[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_10.to_csv(\"task_usage_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\11\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage11 = pd.DataFrame(comb_np_array)\n",
    "task_usage11.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_11 = task_usage11.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_11 = task_usage_11[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_11.to_csv(\"task_usage_11.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\12\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage12 = pd.DataFrame(comb_np_array)\n",
    "task_usage12.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_12 = task_usage12.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_12 = task_usage_12[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_12.to_csv(\"task_usage_12.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\13\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage13 = pd.DataFrame(comb_np_array)\n",
    "task_usage13.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_13 = task_usage13.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_13 = task_usage_13[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_13.to_csv(\"task_usage_13.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\14\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage14 = pd.DataFrame(comb_np_array)\n",
    "task_usage14.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_14 = task_usage14.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_14 = task_usage_14[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_14.to_csv(\"task_usage_14.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\15\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage15 = pd.DataFrame(comb_np_array)\n",
    "task_usage15.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_15 = task_usage15.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_15 = task_usage_15[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_15.to_csv(\"task_usage_15.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\16\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage16 = pd.DataFrame(comb_np_array)\n",
    "task_usage16.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_16 = task_usage16.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_16 = task_usage_16[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_16.to_csv(\"task_usage_16.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\17\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage17 = pd.DataFrame(comb_np_array)\n",
    "task_usage17.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_17 = task_usage17.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_17 = task_usage_17[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_17.to_csv(\"task_usage_17.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\18\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage18 = pd.DataFrame(comb_np_array)\n",
    "task_usage18.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_18 = task_usage18.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_18 = task_usage_18[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_18.to_csv(\"task_usage_18.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\19\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage19 = pd.DataFrame(comb_np_array)\n",
    "task_usage19.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_19 = task_usage19.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_19 = task_usage_19[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_19.to_csv(\"task_usage_19.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\20\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage20 = pd.DataFrame(comb_np_array)\n",
    "task_usage20.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_20 = task_usage20.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_20 = task_usage_20[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_20.to_csv(\"task_usage_20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\21\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage21 = pd.DataFrame(comb_np_array)\n",
    "task_usage21.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_21 = task_usage21.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_21 = task_usage_21[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_21.to_csv(\"task_usage_21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\22\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage22 = pd.DataFrame(comb_np_array)\n",
    "task_usage22.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_22 = task_usage22.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_22 = task_usage_22[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_22.to_csv(\"task_usage_22.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\23\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage23 = pd.DataFrame(comb_np_array)\n",
    "task_usage23.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_23 = task_usage23.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_23 = task_usage_23[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_23.to_csv(\"task_usage_23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\24\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage24 = pd.DataFrame(comb_np_array)\n",
    "task_usage24.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_24 = task_usage24.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_24 = task_usage_24[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_24.to_csv(\"task_usage_24.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\25\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage25 = pd.DataFrame(comb_np_array)\n",
    "task_usage25.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_25 = task_usage25.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_25 = task_usage_25[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_25.to_csv(\"task_usage_25.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\26\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage26 = pd.DataFrame(comb_np_array)\n",
    "task_usage26.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_26 = task_usage26.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_26 = task_usage_26[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_26.to_csv(\"task_usage_26.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\27\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage27 = pd.DataFrame(comb_np_array)\n",
    "task_usage27.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_27 = task_usage27.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_27 = task_usage_27[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_27.to_csv(\"task_usage_27.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\28\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage28 = pd.DataFrame(comb_np_array)\n",
    "task_usage28.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_28 = task_usage28.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_28 = task_usage_28[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_28.to_csv(\"task_usage_28.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\29\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage29 = pd.DataFrame(comb_np_array)\n",
    "task_usage29.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_29 = task_usage29.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_29 = task_usage_29[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_29.to_csv(\"task_usage_29.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\30\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage30 = pd.DataFrame(comb_np_array)\n",
    "task_usage30.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_30 = task_usage30.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_30 = task_usage_30[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_30.to_csv(\"task_usage_30.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\31\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage31 = pd.DataFrame(comb_np_array)\n",
    "task_usage31.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_31 = task_usage31.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_31 = task_usage_31[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_31.to_csv(\"task_usage_31.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\32\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage32 = pd.DataFrame(comb_np_array)\n",
    "task_usage32.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_32 = task_usage32.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_32 = task_usage_32[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_32.to_csv(\"task_usage_32.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\33\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage33 = pd.DataFrame(comb_np_array)\n",
    "task_usage33.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_33 = task_usage33.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_33 = task_usage_33[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_33.to_csv(\"task_usage_33.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\34\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage34 = pd.DataFrame(comb_np_array)\n",
    "task_usage34.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_34 = task_usage34.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_34 = task_usage_34[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_34.to_csv(\"task_usage_34.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\35\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage35 = pd.DataFrame(comb_np_array)\n",
    "task_usage35.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_35 = task_usage35.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_35 = task_usage_35[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_35.to_csv(\"task_usage_35.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\36\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage36 = pd.DataFrame(comb_np_array)\n",
    "task_usage36.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_36 = task_usage36.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_36 = task_usage_36[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_36.to_csv(\"task_usage_36.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\37\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage37 = pd.DataFrame(comb_np_array)\n",
    "task_usage37.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_37 = task_usage37.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_37 = task_usage_37[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_37.to_csv(\"task_usage_37.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\38\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage38 = pd.DataFrame(comb_np_array)\n",
    "task_usage38.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_38 = task_usage38.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_38 = task_usage_38[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_38.to_csv(\"task_usage_38.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\39\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage39 = pd.DataFrame(comb_np_array)\n",
    "task_usage39.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_39 = task_usage39.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_39 = task_usage_39[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_39.to_csv(\"task_usage_39.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\40\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage40 = pd.DataFrame(comb_np_array)\n",
    "task_usage40.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_40 = task_usage40.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_40 = task_usage_40[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_40.to_csv(\"task_usage_40.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\41\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage41 = pd.DataFrame(comb_np_array)\n",
    "task_usage41.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_41 = task_usage41.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_41 = task_usage_41[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_41.to_csv(\"task_usage_41.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\42\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage42 = pd.DataFrame(comb_np_array)\n",
    "task_usage42.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_42 = task_usage42.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_42 = task_usage_42[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_42.to_csv(\"task_usage_42.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\43\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage43 = pd.DataFrame(comb_np_array)\n",
    "task_usage43.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_43 = task_usage43.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_43 = task_usage_43[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_43.to_csv(\"task_usage_43.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\44\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage44 = pd.DataFrame(comb_np_array)\n",
    "task_usage44.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_44 = task_usage44.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_44 = task_usage_44[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_44.to_csv(\"task_usage_44.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\45\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage45 = pd.DataFrame(comb_np_array)\n",
    "task_usage45.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_45 = task_usage45.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_45 = task_usage_45[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_45.to_csv(\"task_usage_45.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\46\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage46 = pd.DataFrame(comb_np_array)\n",
    "task_usage46.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_46 = task_usage46.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_46 = task_usage_46[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_46.to_csv(\"task_usage_46.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\47\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage47 = pd.DataFrame(comb_np_array)\n",
    "task_usage47.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_47 = task_usage47.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_47 = task_usage_47[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_47.to_csv(\"task_usage_47.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\48\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage48 = pd.DataFrame(comb_np_array)\n",
    "task_usage48.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_48 = task_usage48.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_48 = task_usage_48[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_48.to_csv(\"task_usage_48.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\49\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage49 = pd.DataFrame(comb_np_array)\n",
    "task_usage49.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_49 = task_usage49.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_49 = task_usage_49[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_49.to_csv(\"task_usage_49.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\50\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage50 = pd.DataFrame(comb_np_array)\n",
    "task_usage50.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_50 = task_usage50.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_50 = task_usage_50[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_50.to_csv(\"task_usage_50.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\51\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage51 = pd.DataFrame(comb_np_array)\n",
    "task_usage51.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_51 = task_usage51.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_51 = task_usage_51[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_51.to_csv(\"task_usage_51.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\52\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage52 = pd.DataFrame(comb_np_array)\n",
    "task_usage52.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_52 = task_usage52.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_52 = task_usage_52[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_52.to_csv(\"task_usage_52.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\53\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage53 = pd.DataFrame(comb_np_array)\n",
    "task_usage53.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_53 = task_usage53.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_53 = task_usage_53[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_53.to_csv(\"task_usage_53.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\54\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage54 = pd.DataFrame(comb_np_array)\n",
    "task_usage54.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_54 = task_usage54.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_54 = task_usage_54[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_54.to_csv(\"task_usage_54.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\55\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage55 = pd.DataFrame(comb_np_array)\n",
    "task_usage55.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_55 = task_usage55.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_55 = task_usage_55[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_55.to_csv(\"task_usage_55.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\56\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage56 = pd.DataFrame(comb_np_array)\n",
    "task_usage56.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_56 = task_usage56.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_56 = task_usage_56[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_56.to_csv(\"task_usage_56.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\57\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage57 = pd.DataFrame(comb_np_array)\n",
    "task_usage57.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_57 = task_usage57.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_57 = task_usage_57[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_57.to_csv(\"task_usage_57.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\58\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage58 = pd.DataFrame(comb_np_array)\n",
    "task_usage58.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_58 = task_usage58.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_58 = task_usage_58[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_58.to_csv(\"task_usage_58.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\59\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage59 = pd.DataFrame(comb_np_array)\n",
    "task_usage59.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_59 = task_usage59.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_59 = task_usage_59[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_59.to_csv(\"task_usage_59.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\60\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage60 = pd.DataFrame(comb_np_array)\n",
    "task_usage60.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_60 = task_usage60.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_60 = task_usage_60[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_60.to_csv(\"task_usage_60.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\61\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage61 = pd.DataFrame(comb_np_array)\n",
    "task_usage61.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_61 = task_usage61.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_61 = task_usage_61[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_61.to_csv(\"task_usage_61.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\62\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage62 = pd.DataFrame(comb_np_array)\n",
    "task_usage62.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_62 = task_usage62.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_62 = task_usage_62[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_62.to_csv(\"task_usage_62.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\63\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage63 = pd.DataFrame(comb_np_array)\n",
    "task_usage63.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_63 = task_usage63.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_63 = task_usage_63[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_63.to_csv(\"task_usage_63.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\64\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage64 = pd.DataFrame(comb_np_array)\n",
    "task_usage64.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_64 = task_usage64.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_64 = task_usage_64[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_64.to_csv(\"task_usage_64.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\65\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage65 = pd.DataFrame(comb_np_array)\n",
    "task_usage65.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_65 = task_usage65.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_65 = task_usage_65[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_65.to_csv(\"task_usage_65.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\66\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage66 = pd.DataFrame(comb_np_array)\n",
    "task_usage66.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_66 = task_usage66.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_66 = task_usage_66[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_66.to_csv(\"task_usage_66.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\67\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage67 = pd.DataFrame(comb_np_array)\n",
    "task_usage67.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_67 = task_usage67.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_67 = task_usage_67[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_67.to_csv(\"task_usage_67.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\68\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage68 = pd.DataFrame(comb_np_array)\n",
    "task_usage68.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_68 = task_usage68.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_68 = task_usage_68[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_68.to_csv(\"task_usage_68.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\69\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage69 = pd.DataFrame(comb_np_array)\n",
    "task_usage69.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_69 = task_usage69.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_69 = task_usage_69[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_69.to_csv(\"task_usage_69.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\70\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage70 = pd.DataFrame(comb_np_array)\n",
    "task_usage70.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_70 = task_usage70.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_70 = task_usage_70[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_70.to_csv(\"task_usage_70.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\71\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage71 = pd.DataFrame(comb_np_array)\n",
    "task_usage71.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_71 = task_usage71.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_71 = task_usage_71[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_71.to_csv(\"task_usage_71.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\72\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage72 = pd.DataFrame(comb_np_array)\n",
    "task_usage72.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_72 = task_usage72.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_72 = task_usage_72[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_72.to_csv(\"task_usage_72.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\73\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage73 = pd.DataFrame(comb_np_array)\n",
    "task_usage73.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_73 = task_usage73.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_73 = task_usage_73[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_73.to_csv(\"task_usage_73.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\74\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage74 = pd.DataFrame(comb_np_array)\n",
    "task_usage74.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_74 = task_usage74.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_74 = task_usage_74[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_74.to_csv(\"task_usage_74.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\75\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage75 = pd.DataFrame(comb_np_array)\n",
    "task_usage75.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_75 = task_usage75.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_75 = task_usage_75[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_75.to_csv(\"task_usage_75.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\76\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage76 = pd.DataFrame(comb_np_array)\n",
    "task_usage76.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_76 = task_usage76.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_76 = task_usage_76[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_76.to_csv(\"task_usage_76.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\77\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage77 = pd.DataFrame(comb_np_array)\n",
    "task_usage77.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_77 = task_usage77.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_77 = task_usage_77[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_77.to_csv(\"task_usage_77.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\78\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage78 = pd.DataFrame(comb_np_array)\n",
    "task_usage78.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_78 = task_usage78.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_78 = task_usage_78[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_78.to_csv(\"task_usage_78.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\79\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage79 = pd.DataFrame(comb_np_array)\n",
    "task_usage79.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_79 = task_usage79.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_79 = task_usage_79[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_79.to_csv(\"task_usage_79.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\80\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage80 = pd.DataFrame(comb_np_array)\n",
    "task_usage80.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_80 = task_usage80.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_80 = task_usage_80[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_80.to_csv(\"task_usage_80.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\81\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage81 = pd.DataFrame(comb_np_array)\n",
    "task_usage81.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_81 = task_usage81.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_81 = task_usage_81[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_81.to_csv(\"task_usage_81.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\82\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage82 = pd.DataFrame(comb_np_array)\n",
    "task_usage82.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_82 = task_usage82.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_82 = task_usage_82[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_82.to_csv(\"task_usage_82.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\83\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage83 = pd.DataFrame(comb_np_array)\n",
    "task_usage83.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_83 = task_usage83.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_83 = task_usage_83[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_83.to_csv(\"task_usage_83.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\84\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage84 = pd.DataFrame(comb_np_array)\n",
    "task_usage84.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_84 = task_usage84.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_84 = task_usage_84[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_84.to_csv(\"task_usage_84.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\85\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage85 = pd.DataFrame(comb_np_array)\n",
    "task_usage85.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_85 = task_usage85.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_85 = task_usage_85[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_85.to_csv(\"task_usage_85.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\86\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage86 = pd.DataFrame(comb_np_array)\n",
    "task_usage86.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_86 = task_usage86.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_86 = task_usage_86[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_86.to_csv(\"task_usage_86.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\87\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage87 = pd.DataFrame(comb_np_array)\n",
    "task_usage87.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_87 = task_usage87.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_87 = task_usage_87[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_87.to_csv(\"task_usage_87.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\88\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage88 = pd.DataFrame(comb_np_array)\n",
    "task_usage88.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_88 = task_usage88.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_88 = task_usage_88[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_88.to_csv(\"task_usage_88.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\89\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage89 = pd.DataFrame(comb_np_array)\n",
    "task_usage89.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_89 = task_usage89.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_89 = task_usage_89[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_89.to_csv(\"task_usage_89.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\90\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage90 = pd.DataFrame(comb_np_array)\n",
    "task_usage90.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_90 = task_usage90.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_90 = task_usage_90[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_90.to_csv(\"task_usage_90.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\91\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage91 = pd.DataFrame(comb_np_array)\n",
    "task_usage91.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_91 = task_usage91.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_91 = task_usage_91[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_91.to_csv(\"task_usage_91.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\92\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage92 = pd.DataFrame(comb_np_array)\n",
    "task_usage92.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_92 = task_usage92.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_92 = task_usage_92[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_92.to_csv(\"task_usage_92.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\93\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage93 = pd.DataFrame(comb_np_array)\n",
    "task_usage93.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_93 = task_usage93.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_93 = task_usage_93[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_93.to_csv(\"task_usage_93.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\94\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage94 = pd.DataFrame(comb_np_array)\n",
    "task_usage94.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_94 = task_usage94.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_94 = task_usage_94[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_94.to_csv(\"task_usage_94.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\95\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage95 = pd.DataFrame(comb_np_array)\n",
    "task_usage95.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_95 = task_usage95.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_95 = task_usage_95[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_95.to_csv(\"task_usage_95.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\96\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage96 = pd.DataFrame(comb_np_array)\n",
    "task_usage96.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_96 = task_usage96.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_96 = task_usage_96[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_96.to_csv(\"task_usage_96.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\97\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage97 = pd.DataFrame(comb_np_array)\n",
    "task_usage97.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_97 = task_usage97.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_97 = task_usage_97[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_97.to_csv(\"task_usage_97.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\98\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage98 = pd.DataFrame(comb_np_array)\n",
    "task_usage98.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_98 = task_usage98.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_98 = task_usage_98[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_98.to_csv(\"task_usage_98.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\99\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage99 = pd.DataFrame(comb_np_array)\n",
    "task_usage99.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_99 = task_usage99.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_99 = task_usage_99[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_99.to_csv(\"task_usage_99.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\data\\newData\\clusterdata-2011-2\\task_usage\\100\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3,5,6,7])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_usage100 = pd.DataFrame(comb_np_array)\n",
    "task_usage100.columns = ['start time', 'end time', 'job ID', 'task index', 'CPU rate', 'canonical memory usage', 'assigned memory usage']\n",
    "task_usage_100 = task_usage100.dropna(subset=['CPU rate', 'canonical memory usage', 'assigned memory usage'])\n",
    "task_usage_100 = task_usage_100[['start time', 'end time', 'job ID', 'task index']]\n",
    "task_usage_100.to_csv(\"task_usage_100.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count the number of jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_events_type01 = pd.read_csv (r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\job_events_type01.csv',header=None,skiprows=1,usecols = [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_events_type01.columns = [\"job ID\", \"event type\", \"job scheduling class\", \"job name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             job ID  event type  job scheduling class  \\\n",
      "0           3418314           0                     3   \n",
      "1           3418319           0                     3   \n",
      "2           3418324           0                     3   \n",
      "3           3418329           0                     3   \n",
      "4           3418334           0                     3   \n",
      "...             ...         ...                   ...   \n",
      "1343873  6486638013           1                     1   \n",
      "1343874  6486638079           0                     1   \n",
      "1343875  6486638079           1                     1   \n",
      "1343876  6486641236           0                     0   \n",
      "1343877  6486641236           1                     0   \n",
      "\n",
      "                                             job name  \n",
      "0        L52XDyhi9x9ChmVBZ1qavOFmnzPeVsvQ2QyGmBZcV4s=  \n",
      "1        vq0IN3BWEbkDjYgYvkrVyH6OWoUoDwFFf3j/syEZzLA=  \n",
      "2        X+Vce15Yu3BCKb7Ttc6hvINAzdfG3NtYEDNNsPdMGKo=  \n",
      "3        EeK3DUWYi1P0vgBTp7wZdUos8UKj/+/FqudTLohMQ9M=  \n",
      "4        noCrQkR+8CU32ibuNmNvobHFFGuXRZ2aM/ZvcMHaOg4=  \n",
      "...                                               ...  \n",
      "1343873  hUWYVElFOTQFBOvH8c93V3Z7ASi29Mk9Yg4tc5PCTG4=  \n",
      "1343874  9XfxgqyEqcJEDRFyjlU9D0jaPwhgk3S02Eb/ZyCdD3M=  \n",
      "1343875  9XfxgqyEqcJEDRFyjlU9D0jaPwhgk3S02Eb/ZyCdD3M=  \n",
      "1343876  6cEyQ7YEIiUwugpf7HOg87W3QG7RZOrfzsyOwZwMszw=  \n",
      "1343877  6cEyQ7YEIiUwugpf7HOg87W3QG7RZOrfzsyOwZwMszw=  \n",
      "\n",
      "[1343878 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print (job_events_type01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_events_type01.to_csv(\"job_events_type01modified.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job ID\n",
      "3418309       1\n",
      "3418314       2\n",
      "3418319       2\n",
      "3418324       2\n",
      "3418329       2\n",
      "             ..\n",
      "6486633859    2\n",
      "6486634035    2\n",
      "6486638013    2\n",
      "6486638079    2\n",
      "6486641236    2\n",
      "Name: event type, Length: 672074, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "jobnumber = job_events_type01.groupby('job ID')['event type'].count()\n",
    "print (jobnumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672074"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobnumber.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobnumber.to_csv(\"jobnumber.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count the number of tasks for each job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\task_events\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,1,2,3])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "cnt = pd.DataFrame(comb_np_array)\n",
    "cnt.columns = ['job ID', 'task index', 'event type', 'task scheduling class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                job ID  task index  event type  task scheduling class\n",
      "0         6.439339e+09      2722.0         1.0                    0.0\n",
      "1         6.439339e+09      2723.0         1.0                    0.0\n",
      "2         6.439339e+09      2724.0         1.0                    0.0\n",
      "3         6.439339e+09      2725.0         1.0                    0.0\n",
      "4         6.439339e+09      2726.0         1.0                    0.0\n",
      "...                ...         ...         ...                    ...\n",
      "95725671  6.439339e+09      2716.0         1.0                    0.0\n",
      "95725672  6.439339e+09      2717.0         1.0                    0.0\n",
      "95725673  6.439339e+09      2718.0         1.0                    0.0\n",
      "95725674  6.439339e+09      2719.0         1.0                    0.0\n",
      "95725675  6.439339e+09      2720.0         1.0                    0.0\n",
      "\n",
      "[95725676 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print (cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt.to_csv(\"cnt.csv\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_1 = cnt.groupby('job ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_2 = cnt_1.count().reset_index()[['job ID','task index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              job ID  task index\n",
      "0       3.418309e+06           4\n",
      "1       3.418314e+06           5\n",
      "2       3.418319e+06           6\n",
      "3       3.418324e+06           6\n",
      "4       3.418329e+06           9\n",
      "...              ...         ...\n",
      "671999  6.486634e+09           2\n",
      "672000  6.486634e+09           2\n",
      "672001  6.486638e+09           2\n",
      "672002  6.486638e+09         378\n",
      "672003  6.486641e+09           2\n",
      "\n",
      "[672004 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print (cnt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_2.to_csv(\"Number_of_tasks_for_each_job.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the sum of CPU request, memory request for each job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\task_events\"\n",
    "allFiles = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0, usecols = [0,4,5])\n",
    "    np_array_list.append(df)\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "task_event = pd.DataFrame(comb_np_array)\n",
    "task_event.columns = ['job ID', 'task CPU request', 'task memory request']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                job ID  task CPU request  task memory request\n",
      "0         6.439339e+09            0.0625              0.02386\n",
      "1         6.439339e+09            0.0625              0.02386\n",
      "2         6.439339e+09            0.0625              0.02386\n",
      "3         6.439339e+09            0.0625              0.02386\n",
      "4         6.439339e+09            0.0625              0.02386\n",
      "...                ...               ...                  ...\n",
      "95725671  6.439339e+09            0.0625              0.02386\n",
      "95725672  6.439339e+09            0.0625              0.02386\n",
      "95725673  6.439339e+09            0.0625              0.02386\n",
      "95725674  6.439339e+09            0.0625              0.02386\n",
      "95725675  6.439339e+09            0.0625              0.02386\n",
      "\n",
      "[95725676 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print (task_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_event_1 = task_event.groupby('job ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_event_2 = task_event_1.sum().reset_index()[['job ID', 'task CPU request', 'task memory request']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              job ID  task CPU request  task memory request\n",
      "0       3.418309e+06           0.50000             0.297840\n",
      "1       3.418314e+06           0.37500             0.453380\n",
      "2       3.418319e+06           0.18750             0.521460\n",
      "3       3.418324e+06           0.37500             0.528780\n",
      "4       3.418329e+06           1.12500             0.893160\n",
      "...              ...               ...                  ...\n",
      "671999  6.486634e+09           0.03124             0.031060\n",
      "672000  6.486634e+09           0.03124             0.031060\n",
      "672001  6.486638e+09           0.03124             0.031060\n",
      "672002  6.486638e+09          25.97994             7.219800\n",
      "672003  6.486641e+09           0.00000             0.000311\n",
      "\n",
      "[672004 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print (task_event_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_event_2.to_csv(\"Sum_of_CPU_request_memory_request_for_each_job.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_events_type01modified = pd.read_csv (r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\job_events_type01modified.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_events_type01modified.columns = [\"job ID\", \"event type\", \"job scheduling class\", \"job name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             job ID  event type  job scheduling class  \\\n",
      "0           3418314           0                     3   \n",
      "1           3418319           0                     3   \n",
      "2           3418324           0                     3   \n",
      "3           3418329           0                     3   \n",
      "4           3418334           0                     3   \n",
      "...             ...         ...                   ...   \n",
      "1343873  6486638013           1                     1   \n",
      "1343874  6486638079           0                     1   \n",
      "1343875  6486638079           1                     1   \n",
      "1343876  6486641236           0                     0   \n",
      "1343877  6486641236           1                     0   \n",
      "\n",
      "                                             job name  \n",
      "0        L52XDyhi9x9ChmVBZ1qavOFmnzPeVsvQ2QyGmBZcV4s=  \n",
      "1        vq0IN3BWEbkDjYgYvkrVyH6OWoUoDwFFf3j/syEZzLA=  \n",
      "2        X+Vce15Yu3BCKb7Ttc6hvINAzdfG3NtYEDNNsPdMGKo=  \n",
      "3        EeK3DUWYi1P0vgBTp7wZdUos8UKj/+/FqudTLohMQ9M=  \n",
      "4        noCrQkR+8CU32ibuNmNvobHFFGuXRZ2aM/ZvcMHaOg4=  \n",
      "...                                               ...  \n",
      "1343873  hUWYVElFOTQFBOvH8c93V3Z7ASi29Mk9Yg4tc5PCTG4=  \n",
      "1343874  9XfxgqyEqcJEDRFyjlU9D0jaPwhgk3S02Eb/ZyCdD3M=  \n",
      "1343875  9XfxgqyEqcJEDRFyjlU9D0jaPwhgk3S02Eb/ZyCdD3M=  \n",
      "1343876  6cEyQ7YEIiUwugpf7HOg87W3QG7RZOrfzsyOwZwMszw=  \n",
      "1343877  6cEyQ7YEIiUwugpf7HOg87W3QG7RZOrfzsyOwZwMszw=  \n",
      "\n",
      "[1343878 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print (job_events_type01modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events_1modified = pd.read_csv (r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\task_events\\task_events_1modified.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_events_1modified.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               job ID  task index  event type  task scheduling class  \\\n",
      "0        3.418314e+06         0.0         0.0                    3.0   \n",
      "1        3.418314e+06         1.0         0.0                    3.0   \n",
      "2        3.418368e+06         0.0         0.0                    3.0   \n",
      "3        3.418368e+06         1.0         0.0                    3.0   \n",
      "4        3.418368e+06         2.0         0.0                    3.0   \n",
      "...               ...         ...         ...                    ...   \n",
      "9977433  6.146435e+09        37.0         1.0                    3.0   \n",
      "9977434  6.266780e+09         0.0         0.0                    2.0   \n",
      "9977435  5.150430e+08        14.0         0.0                    2.0   \n",
      "9977436  5.150430e+08        18.0         0.0                    2.0   \n",
      "9977437  5.150430e+08        24.0         0.0                    2.0   \n",
      "\n",
      "         task CPU request  task memory request  task disk space request  \n",
      "0                0.125000              0.07446                 0.000424  \n",
      "1                0.125000              0.07446                 0.000424  \n",
      "2                0.031250              0.08691                 0.000455  \n",
      "3                0.031250              0.08691                 0.000455  \n",
      "4                0.031250              0.08691                 0.000455  \n",
      "...                   ...                  ...                      ...  \n",
      "9977433          0.062500              0.11740                 0.000623  \n",
      "9977434          0.006248              0.03107                 0.000038  \n",
      "9977435          0.015620              0.01553                 0.000216  \n",
      "9977436          0.015620              0.01553                 0.000216  \n",
      "9977437          0.015620              0.01553                 0.000216  \n",
      "\n",
      "[9977438 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print (task_events_1modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_1 = pd.merge(job_events_type01modified, task_events_1modified, on = ['job ID', 'event type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_1 = merge_1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             job ID  event type  job scheduling class  \\\n",
      "0           3418314           0                     3   \n",
      "1           3418314           0                     3   \n",
      "2           3418339           0                     3   \n",
      "3           3418368           0                     3   \n",
      "4           3418368           0                     3   \n",
      "...             ...         ...                   ...   \n",
      "9977198  6266779253           0                     0   \n",
      "9977199  6266779253           0                     0   \n",
      "9977200  6266779253           0                     0   \n",
      "9977201  6266779253           0                     0   \n",
      "9977202  6266779829           0                     2   \n",
      "\n",
      "                                             job name  task index  \\\n",
      "0        L52XDyhi9x9ChmVBZ1qavOFmnzPeVsvQ2QyGmBZcV4s=         0.0   \n",
      "1        L52XDyhi9x9ChmVBZ1qavOFmnzPeVsvQ2QyGmBZcV4s=         1.0   \n",
      "2        ORioZ5deSIAxIuzqmXo1Ivac+22YZbCucJEC0EqRbDc=        16.0   \n",
      "3        mIF6r5X6TTau4MPuBTE+QevbEjjACcfyVeRTAU79lpg=         0.0   \n",
      "4        mIF6r5X6TTau4MPuBTE+QevbEjjACcfyVeRTAU79lpg=         1.0   \n",
      "...                                               ...         ...   \n",
      "9977198  8ai311EyOz/z0cZElYSLlMnu6iyoD6dLblnhAWvzXrQ=       494.0   \n",
      "9977199  8ai311EyOz/z0cZElYSLlMnu6iyoD6dLblnhAWvzXrQ=       495.0   \n",
      "9977200  8ai311EyOz/z0cZElYSLlMnu6iyoD6dLblnhAWvzXrQ=       496.0   \n",
      "9977201  8ai311EyOz/z0cZElYSLlMnu6iyoD6dLblnhAWvzXrQ=       497.0   \n",
      "9977202  DBEIR10/vkBZIuFJB7N9W71NZbTRdSaxxBxwIMfCkSY=         0.0   \n",
      "\n",
      "         task scheduling class  task CPU request  task memory request  \\\n",
      "0                          3.0          0.125000              0.07446   \n",
      "1                          3.0          0.125000              0.07446   \n",
      "2                          3.0          0.187500              0.09839   \n",
      "3                          3.0          0.031250              0.08691   \n",
      "4                          3.0          0.031250              0.08691   \n",
      "...                        ...               ...                  ...   \n",
      "9977198                    0.0          0.031250              0.03107   \n",
      "9977199                    0.0          0.031250              0.03107   \n",
      "9977200                    0.0          0.031250              0.03107   \n",
      "9977201                    0.0          0.031250              0.03107   \n",
      "9977202                    2.0          0.006248              0.03107   \n",
      "\n",
      "         task disk space request  \n",
      "0                       0.000424  \n",
      "1                       0.000424  \n",
      "2                       0.000861  \n",
      "3                       0.000455  \n",
      "4                       0.000455  \n",
      "...                          ...  \n",
      "9977198                 0.001131  \n",
      "9977199                 0.001131  \n",
      "9977200                 0.001131  \n",
      "9977201                 0.001131  \n",
      "9977202                 0.000038  \n",
      "\n",
      "[9977203 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print (merge_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_1.to_csv(\"merge_1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             job ID  event type  job scheduling class  \\\n",
      "0           3418319           0                     3   \n",
      "1           3418339           0                     3   \n",
      "2           3418375           0                     3   \n",
      "3           3418390           0                     3   \n",
      "4           6590371           0                     3   \n",
      "...             ...         ...                   ...   \n",
      "5931815  6280836459           1                     1   \n",
      "5931816  6280836459           1                     1   \n",
      "5931817  6280836459           1                     1   \n",
      "5931818  6280836459           1                     1   \n",
      "5931819  6280836459           1                     1   \n",
      "\n",
      "                                             job name  task index  \\\n",
      "0        vq0IN3BWEbkDjYgYvkrVyH6OWoUoDwFFf3j/syEZzLA=         0.0   \n",
      "1        ORioZ5deSIAxIuzqmXo1Ivac+22YZbCucJEC0EqRbDc=        17.0   \n",
      "2        inYBvTIcGwrlGypZxG2zdn08Txy0qhqciqH1g5YMFDY=         0.0   \n",
      "3        LsgmVCh2BpMrFrIsSDs+J7V3m0Cszc1/3zo9pfMhIC0=         4.0   \n",
      "4        tOGGV9YnzwN8kCzYAUGSxSvOYsiPXlETXyEGthCvhtg=         1.0   \n",
      "...                                               ...         ...   \n",
      "5931815  hkuOO2fBEghUe7yep0dDDVle06K4D3CBQaNIDRFyloc=         4.0   \n",
      "5931816  hkuOO2fBEghUe7yep0dDDVle06K4D3CBQaNIDRFyloc=         5.0   \n",
      "5931817  hkuOO2fBEghUe7yep0dDDVle06K4D3CBQaNIDRFyloc=         6.0   \n",
      "5931818  hkuOO2fBEghUe7yep0dDDVle06K4D3CBQaNIDRFyloc=         7.0   \n",
      "5931819  hkuOO2fBEghUe7yep0dDDVle06K4D3CBQaNIDRFyloc=         8.0   \n",
      "\n",
      "         task scheduling class  task CPU request  task memory request  \\\n",
      "0                          3.0           0.03125              0.08691   \n",
      "1                          3.0           0.18750              0.09839   \n",
      "2                          3.0           0.06250              0.09277   \n",
      "3                          3.0           0.06250              0.12240   \n",
      "4                          3.0           0.03125              0.08765   \n",
      "...                        ...               ...                  ...   \n",
      "5931815                    1.0           0.06250              0.03979   \n",
      "5931816                    1.0           0.06250              0.03979   \n",
      "5931817                    1.0           0.06250              0.03979   \n",
      "5931818                    1.0           0.06250              0.03979   \n",
      "5931819                    1.0           0.06250              0.03979   \n",
      "\n",
      "         task disk space request  \n",
      "0                       0.000455  \n",
      "1                       0.000861  \n",
      "2                       0.000494  \n",
      "3                       0.000594  \n",
      "4                       0.000470  \n",
      "...                          ...  \n",
      "5931815                 0.000077  \n",
      "5931816                 0.000077  \n",
      "5931817                 0.000077  \n",
      "5931818                 0.000077  \n",
      "5931819                 0.000077  \n",
      "\n",
      "[5931820 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "task_events_2modified = pd.read_csv (r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\task_events\\task_events_2modified.csv',header=None)\n",
    "task_events_2modified.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request'] \n",
    "merge_2 = pd.merge(job_events_type01modified, task_events_2modified, on = ['job ID', 'event type'])\n",
    "merge_2 = merge_2.dropna()\n",
    "print (merge_2)\n",
    "merge_2.to_csv(\"merge_2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             job ID  event type  job scheduling class  \\\n",
      "0           3418339           0                     3   \n",
      "1           6590386           0                     3   \n",
      "2           7053513           0                     3   \n",
      "3          17109330           0                     3   \n",
      "4          17109330           0                     3   \n",
      "...             ...         ...                   ...   \n",
      "6046751  6300426908           1                     1   \n",
      "6046752  6300426992           0                     1   \n",
      "6046753  6300426992           1                     1   \n",
      "6046754  6300390683           0                     1   \n",
      "6046755  6300390683           1                     1   \n",
      "\n",
      "                                             job name  task index  \\\n",
      "0        ORioZ5deSIAxIuzqmXo1Ivac+22YZbCucJEC0EqRbDc=        17.0   \n",
      "1        6VSzxIfbdDCDaNqQZz6XeMCphC0UiZ5X1e1EpkYQQNM=         0.0   \n",
      "2        CGA5JG7Xqic9WN0OKIW//Y3soLlfZTMOlVEdK0gXYdM=         2.0   \n",
      "3        NJkTVcgXsCnD/T38G6X7vRjtuzZhtmipJETxt6y1mpE=       138.0   \n",
      "4        NJkTVcgXsCnD/T38G6X7vRjtuzZhtmipJETxt6y1mpE=       150.0   \n",
      "...                                               ...         ...   \n",
      "6046751  OLXv0Uy63d4h60w+3QB1CTlTzYGF/VjeeI4ACFq4Hjw=         0.0   \n",
      "6046752  f4wsyU/mcU2gD/zsy1GKRl6Blvwz5pdrXaSwHkW1PGM=         0.0   \n",
      "6046753  f4wsyU/mcU2gD/zsy1GKRl6Blvwz5pdrXaSwHkW1PGM=         0.0   \n",
      "6046754  rJe+HgfopjKiGAi44YrYH5dmEpPBi0VSYKJe+p5jaow=         0.0   \n",
      "6046755  rJe+HgfopjKiGAi44YrYH5dmEpPBi0VSYKJe+p5jaow=         0.0   \n",
      "\n",
      "         task scheduling class  task CPU request  task memory request  \\\n",
      "0                          3.0          0.187500             0.098390   \n",
      "1                          3.0          0.031250             0.086910   \n",
      "2                          3.0          0.031250             0.003109   \n",
      "3                          3.0          0.062500             0.013980   \n",
      "4                          3.0          0.062500             0.013980   \n",
      "...                        ...               ...                  ...   \n",
      "6046751                    1.0          0.006248             0.015530   \n",
      "6046752                    1.0          0.006248             0.015530   \n",
      "6046753                    1.0          0.006248             0.015530   \n",
      "6046754                    1.0          0.006248             0.031800   \n",
      "6046755                    1.0          0.006248             0.031800   \n",
      "\n",
      "         task disk space request  \n",
      "0                       0.000861  \n",
      "1                       0.000470  \n",
      "2                       0.000038  \n",
      "3                       0.000672  \n",
      "4                       0.000672  \n",
      "...                          ...  \n",
      "6046751                 0.000038  \n",
      "6046752                 0.000038  \n",
      "6046753                 0.000038  \n",
      "6046754                 0.000045  \n",
      "6046755                 0.000045  \n",
      "\n",
      "[6046756 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "task_events_3modified = pd.read_csv (r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\task_events\\task_events_3modified.csv',header=None)\n",
    "task_events_3modified.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request'] \n",
    "merge_3 = pd.merge(job_events_type01modified, task_events_3modified, on = ['job ID', 'event type'])\n",
    "merge_3 = merge_3.dropna()\n",
    "print (merge_3)\n",
    "merge_3.to_csv(\"merge_3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              job ID  event type  job scheduling class  \\\n",
      "0            3418319           0                     3   \n",
      "1            3418324           0                     3   \n",
      "2            3418324           0                     3   \n",
      "3            3418329           0                     3   \n",
      "4            3418339           0                     3   \n",
      "...              ...         ...                   ...   \n",
      "13144354  6328104534           1                     0   \n",
      "13144355  6328111279           0                     2   \n",
      "13144356  6328111279           1                     2   \n",
      "13144357  6328114801           0                     2   \n",
      "13144358  6328114801           1                     2   \n",
      "\n",
      "                                              job name  task index  \\\n",
      "0         vq0IN3BWEbkDjYgYvkrVyH6OWoUoDwFFf3j/syEZzLA=         0.0   \n",
      "1         X+Vce15Yu3BCKb7Ttc6hvINAzdfG3NtYEDNNsPdMGKo=         0.0   \n",
      "2         X+Vce15Yu3BCKb7Ttc6hvINAzdfG3NtYEDNNsPdMGKo=         0.0   \n",
      "3         EeK3DUWYi1P0vgBTp7wZdUos8UKj/+/FqudTLohMQ9M=         0.0   \n",
      "4         ORioZ5deSIAxIuzqmXo1Ivac+22YZbCucJEC0EqRbDc=        16.0   \n",
      "...                                                ...         ...   \n",
      "13144354  2I28RlP4N63ZQyGYYezfzEsG7FausEB3wHXinegJ8BM=         0.0   \n",
      "13144355  zXaLi/Cte76uGniz5uZVz42HFFtGKLugEc+HUv5QG6A=         0.0   \n",
      "13144356  zXaLi/Cte76uGniz5uZVz42HFFtGKLugEc+HUv5QG6A=         0.0   \n",
      "13144357  CVZfk+UiTH6LBOohQ+NDqc9FSPdTs4SihN8jK3JzkW4=         0.0   \n",
      "13144358  CVZfk+UiTH6LBOohQ+NDqc9FSPdTs4SihN8jK3JzkW4=         0.0   \n",
      "\n",
      "          task scheduling class  task CPU request  task memory request  \\\n",
      "0                           3.0           0.03125              0.08691   \n",
      "1                           3.0           0.06250              0.08813   \n",
      "2                           3.0           0.06250              0.08813   \n",
      "3                           3.0           0.12500              0.09924   \n",
      "4                           3.0           0.18750              0.09839   \n",
      "...                         ...               ...                  ...   \n",
      "13144354                    0.0           0.01250              0.02795   \n",
      "13144355                    2.0           0.01250              0.02945   \n",
      "13144356                    2.0           0.01250              0.02945   \n",
      "13144357                    2.0           0.06250              0.15920   \n",
      "13144358                    2.0           0.06250              0.15920   \n",
      "\n",
      "          task disk space request  \n",
      "0                        0.000455  \n",
      "1                        0.000470  \n",
      "2                        0.000470  \n",
      "3                        0.001049  \n",
      "4                        0.000861  \n",
      "...                           ...  \n",
      "13144354                 0.000261  \n",
      "13144355                 0.000174  \n",
      "13144356                 0.000174  \n",
      "13144357                 0.000347  \n",
      "13144358                 0.000347  \n",
      "\n",
      "[13144359 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "task_events_4modified = pd.read_csv (r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\task_events\\task_events_4modified.csv',header=None)\n",
    "task_events_4modified.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request'] \n",
    "merge_4 = pd.merge(job_events_type01modified, task_events_4modified, on = ['job ID', 'event type'])\n",
    "merge_4 = merge_4.dropna()\n",
    "print (merge_4)\n",
    "merge_4.to_csv(\"merge_4.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              job ID  event type  job scheduling class  \\\n",
      "0            3418339           0                     3   \n",
      "1            3418356           0                     3   \n",
      "2            3418405           0                     3   \n",
      "3            6933059           0                     2   \n",
      "4           17109330           0                     3   \n",
      "...              ...         ...                   ...   \n",
      "11696304  6350360351           1                     0   \n",
      "11696305  6350359792           0                     1   \n",
      "11696306  6350359831           0                     1   \n",
      "11696307  6350359930           0                     1   \n",
      "11696308  6350359982           0                     1   \n",
      "\n",
      "                                              job name  task index  \\\n",
      "0         ORioZ5deSIAxIuzqmXo1Ivac+22YZbCucJEC0EqRbDc=        23.0   \n",
      "1         QGf5Bi+7GCnOYjucoJ9xKNPFf9bWGxoEA+M1A1JPvbQ=         4.0   \n",
      "2         BYQh99FwV+DE3W9a1npuYBtyFK5iTHZpTH6vuu6C+w8=         3.0   \n",
      "3         T5tUMNr1ViKWCvjepVidokYkMfbMDvf0EOlvL2cVBUk=         0.0   \n",
      "4         NJkTVcgXsCnD/T38G6X7vRjtuzZhtmipJETxt6y1mpE=       119.0   \n",
      "...                                                ...         ...   \n",
      "11696304  6cEyQ7YEIiUwugpf7HOg87W3QG7RZOrfzsyOwZwMszw=         0.0   \n",
      "11696305  GiC13UxhX+vRwCBPF+5CJB1CJjJJeGHK0vgJvi+mEcY=         0.0   \n",
      "11696306  8A9C90oTolRZ2o+AOdgx+QH76qoWQsA9m9w1F7W/uoQ=         0.0   \n",
      "11696307  7qTbtB6WW1bbgA8SPfCK+ZyNuf4ZG5iWirHhBzIGUhw=         0.0   \n",
      "11696308  DLjKxXWEVaAstjx+zx0hxyO2OCSgY8GQtYjb9xEVXEo=         0.0   \n",
      "\n",
      "          task scheduling class  task CPU request  task memory request  \\\n",
      "0                           3.0          0.187500             0.098390   \n",
      "1                           3.0          0.187500             0.089480   \n",
      "2                           3.0          0.187500             0.091060   \n",
      "3                           2.0          0.078120             0.047360   \n",
      "4                           3.0          0.062500             0.013980   \n",
      "...                         ...               ...                  ...   \n",
      "11696304                    0.0          0.000000             0.000155   \n",
      "11696305                    1.0          0.003124             0.011660   \n",
      "11696306                    1.0          0.006248             0.011660   \n",
      "11696307                    1.0          0.006248             0.004662   \n",
      "11696308                    1.0          0.006248             0.000933   \n",
      "\n",
      "          task disk space request  \n",
      "0                        0.000861  \n",
      "1                        0.000829  \n",
      "2                        0.000829  \n",
      "3                        0.000154  \n",
      "4                        0.000672  \n",
      "...                           ...  \n",
      "11696304                 0.000000  \n",
      "11696305                 0.000019  \n",
      "11696306                 0.000019  \n",
      "11696307                 0.000019  \n",
      "11696308                 0.000002  \n",
      "\n",
      "[11696309 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "task_events_5modified = pd.read_csv (r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\task_events\\task_events_5modified.csv',header=None)\n",
    "task_events_5modified.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request'] \n",
    "merge_5 = pd.merge(job_events_type01modified, task_events_5modified, on = ['job ID', 'event type'])\n",
    "merge_5 = merge_5.dropna()\n",
    "print (merge_5)\n",
    "merge_5.to_csv(\"merge_5.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             job ID  event type  job scheduling class  \\\n",
      "0           3418339           0                     3   \n",
      "1           3418385           0                     3   \n",
      "2           3418405           0                     3   \n",
      "3           3418427           0                     3   \n",
      "4           6590381           0                     3   \n",
      "...             ...         ...                   ...   \n",
      "9269600  6367207742           1                     0   \n",
      "9269601  6367207879           1                     2   \n",
      "9269602  6367209054           0                     0   \n",
      "9269603  6367209054           1                     0   \n",
      "9269604  6367168972           0                     1   \n",
      "\n",
      "                                             job name  task index  \\\n",
      "0        ORioZ5deSIAxIuzqmXo1Ivac+22YZbCucJEC0EqRbDc=         6.0   \n",
      "1        k+zdUudIbBQDDLITJLDOG6tynb7k+wBVZgnHyiI2+WU=         0.0   \n",
      "2        BYQh99FwV+DE3W9a1npuYBtyFK5iTHZpTH6vuu6C+w8=         2.0   \n",
      "3        RB25Bj2iXSw+NamFKkPr2ivrWzGtBQFUhC7hscqDEWE=         1.0   \n",
      "4        oaf8Z8RQqb9mgZvYwSOxWBfynR10y3bL7NzyG8L1bZ4=         0.0   \n",
      "...                                               ...         ...   \n",
      "9269600  dZZ/xxgbzFlRm7t0eAvl1eOXmjuYQF8TL3pmCWKBfEg=         8.0   \n",
      "9269601  tW165Yo5PURIt3Gfp6gtkblZFRp/dmlh34vkM5eWjVA=         0.0   \n",
      "9269602  6cEyQ7YEIiUwugpf7HOg87W3QG7RZOrfzsyOwZwMszw=         0.0   \n",
      "9269603  6cEyQ7YEIiUwugpf7HOg87W3QG7RZOrfzsyOwZwMszw=         0.0   \n",
      "9269604  fKxbiSoANghFgtDftKxKumZSK53sqxEvqkVL4xrTG3I=         0.0   \n",
      "\n",
      "         task scheduling class  task CPU request  task memory request  \\\n",
      "0                          3.0          0.187500             0.098390   \n",
      "1                          3.0          0.125000             0.086910   \n",
      "2                          3.0          0.187500             0.091060   \n",
      "3                          3.0          0.062500             0.090580   \n",
      "4                          3.0          0.031250             0.077640   \n",
      "...                        ...               ...                  ...   \n",
      "9269600                    0.0          0.006248             0.002205   \n",
      "9269601                    2.0          0.006248             0.004662   \n",
      "9269602                    0.0          0.000000             0.000155   \n",
      "9269603                    0.0          0.000000             0.000155   \n",
      "9269604                    1.0          0.006248             0.002796   \n",
      "\n",
      "         task disk space request  \n",
      "0                       0.000861  \n",
      "1                       0.000470  \n",
      "2                       0.000829  \n",
      "3                       0.000821  \n",
      "4                       0.000432  \n",
      "...                          ...  \n",
      "9269600                 0.000038  \n",
      "9269601                 0.000038  \n",
      "9269602                 0.000000  \n",
      "9269603                 0.000000  \n",
      "9269604                 0.000045  \n",
      "\n",
      "[9269605 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "task_events_6modified = pd.read_csv (r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\task_events\\task_events_6modified.csv',header=None)\n",
    "task_events_6modified.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request'] \n",
    "merge_6 = pd.merge(job_events_type01modified, task_events_6modified, on = ['job ID', 'event type'])\n",
    "merge_6 = merge_6.dropna()\n",
    "print (merge_6)\n",
    "merge_6.to_csv(\"merge_6.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              job ID  event type  job scheduling class  \\\n",
      "0            3418329           0                     3   \n",
      "1            3418339           0                     3   \n",
      "2            3418339           0                     3   \n",
      "3            3418390           0                     3   \n",
      "4            3418390           0                     3   \n",
      "...              ...         ...                   ...   \n",
      "10025841  6392688026           1                     0   \n",
      "10025842  6392688026           1                     0   \n",
      "10025843  6392688026           1                     0   \n",
      "10025844  6392688026           1                     0   \n",
      "10025845  6392688026           1                     0   \n",
      "\n",
      "                                              job name  task index  \\\n",
      "0         EeK3DUWYi1P0vgBTp7wZdUos8UKj/+/FqudTLohMQ9M=         2.0   \n",
      "1         ORioZ5deSIAxIuzqmXo1Ivac+22YZbCucJEC0EqRbDc=         4.0   \n",
      "2         ORioZ5deSIAxIuzqmXo1Ivac+22YZbCucJEC0EqRbDc=         4.0   \n",
      "3         LsgmVCh2BpMrFrIsSDs+J7V3m0Cszc1/3zo9pfMhIC0=         3.0   \n",
      "4         LsgmVCh2BpMrFrIsSDs+J7V3m0Cszc1/3zo9pfMhIC0=         3.0   \n",
      "...                                                ...         ...   \n",
      "10025841  jmMqS18tqIO5u2jtf5rt7Ld3/Umv4lagJS5DxStyLPo=        18.0   \n",
      "10025842  jmMqS18tqIO5u2jtf5rt7Ld3/Umv4lagJS5DxStyLPo=        19.0   \n",
      "10025843  jmMqS18tqIO5u2jtf5rt7Ld3/Umv4lagJS5DxStyLPo=        20.0   \n",
      "10025844  jmMqS18tqIO5u2jtf5rt7Ld3/Umv4lagJS5DxStyLPo=        21.0   \n",
      "10025845  jmMqS18tqIO5u2jtf5rt7Ld3/Umv4lagJS5DxStyLPo=        22.0   \n",
      "\n",
      "          task scheduling class  task CPU request  task memory request  \\\n",
      "0                           3.0          0.125000              0.09924   \n",
      "1                           3.0          0.187500              0.09839   \n",
      "2                           3.0          0.187500              0.09839   \n",
      "3                           3.0          0.062500              0.12240   \n",
      "4                           3.0          0.062500              0.12240   \n",
      "...                         ...               ...                  ...   \n",
      "10025841                    0.0          0.009369              0.01048   \n",
      "10025842                    0.0          0.009369              0.01048   \n",
      "10025843                    0.0          0.009369              0.01048   \n",
      "10025844                    0.0          0.009369              0.01048   \n",
      "10025845                    0.0          0.009369              0.01048   \n",
      "\n",
      "          task disk space request  \n",
      "0                        0.001049  \n",
      "1                        0.000861  \n",
      "2                        0.000861  \n",
      "3                        0.000594  \n",
      "4                        0.000594  \n",
      "...                           ...  \n",
      "10025841                 0.000017  \n",
      "10025842                 0.000017  \n",
      "10025843                 0.000017  \n",
      "10025844                 0.000017  \n",
      "10025845                 0.000017  \n",
      "\n",
      "[10025846 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "task_events_7modified = pd.read_csv (r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\task_events\\task_events_7modified.csv',header=None)\n",
    "task_events_7modified.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request'] \n",
    "merge_7 = pd.merge(job_events_type01modified, task_events_7modified, on = ['job ID', 'event type'])\n",
    "merge_7 = merge_7.dropna()\n",
    "print (merge_7)\n",
    "merge_7.to_csv(\"merge_7.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             job ID  event type  job scheduling class  \\\n",
      "0           3418339           0                     3   \n",
      "1           3418339           0                     3   \n",
      "2           3418368           0                     3   \n",
      "3           3418437           0                     3   \n",
      "4           3418442           0                     3   \n",
      "...             ...         ...                   ...   \n",
      "7451507  6416687741           1                     2   \n",
      "7451508  6416688159           0                     0   \n",
      "7451509  6416678710           0                     1   \n",
      "7451510  6416678710           1                     1   \n",
      "7451511  6416688159           1                     0   \n",
      "\n",
      "                                             job name  task index  \\\n",
      "0        ORioZ5deSIAxIuzqmXo1Ivac+22YZbCucJEC0EqRbDc=        19.0   \n",
      "1        ORioZ5deSIAxIuzqmXo1Ivac+22YZbCucJEC0EqRbDc=        12.0   \n",
      "2        mIF6r5X6TTau4MPuBTE+QevbEjjACcfyVeRTAU79lpg=         1.0   \n",
      "3        NuX0T/LmrGMDmi/hoJuMboS02hqQHdsRbUZxdtJ3Njc=         1.0   \n",
      "4        agg+iJS8cS2LkWyWggO9QYlkA6jbzCoqnOMbmTUk+G0=         4.0   \n",
      "...                                               ...         ...   \n",
      "7451507  aooVLhky98Hvzrvek0uHJVrpiG/T64405SdKVcrweOc=         0.0   \n",
      "7451508  UL7Q5PiMJR25SpsZETqi5pvOTujRL32HbIRwLulmEdk=         0.0   \n",
      "7451509  pELYL+OBJv7T9QXotZAtlmcgRrRxRrx9pKm8L2oQeBs=         0.0   \n",
      "7451510  pELYL+OBJv7T9QXotZAtlmcgRrRxRrx9pKm8L2oQeBs=         0.0   \n",
      "7451511  UL7Q5PiMJR25SpsZETqi5pvOTujRL32HbIRwLulmEdk=         0.0   \n",
      "\n",
      "         task scheduling class  task CPU request  task memory request  \\\n",
      "0                          3.0          0.187500             0.098390   \n",
      "1                          3.0          0.187500             0.098390   \n",
      "2                          3.0          0.125000             0.090210   \n",
      "3                          3.0          0.062500             0.124000   \n",
      "4                          3.0          0.187500             0.015900   \n",
      "...                        ...               ...                  ...   \n",
      "7451507                    2.0          0.006248             0.015900   \n",
      "7451508                    0.0          0.000000             0.000155   \n",
      "7451509                    1.0          0.006248             0.002796   \n",
      "7451510                    1.0          0.006248             0.002796   \n",
      "7451511                    0.0          0.000000             0.000155   \n",
      "\n",
      "         task disk space request  \n",
      "0                       0.000861  \n",
      "1                       0.000861  \n",
      "2                       0.000821  \n",
      "3                       0.000545  \n",
      "4                       0.000071  \n",
      "...                          ...  \n",
      "7451507                 0.000038  \n",
      "7451508                 0.000000  \n",
      "7451509                 0.000045  \n",
      "7451510                 0.000045  \n",
      "7451511                 0.000000  \n",
      "\n",
      "[7451512 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "task_events_8modified = pd.read_csv (r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\task_events\\task_events_8modified.csv',header=None)\n",
    "task_events_8modified.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request'] \n",
    "merge_8 = pd.merge(job_events_type01modified, task_events_8modified, on = ['job ID', 'event type'])\n",
    "merge_8 = merge_8.dropna()\n",
    "print (merge_8)\n",
    "merge_8.to_csv(\"merge_8.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             job ID  event type  job scheduling class  \\\n",
      "0           3418339           0                     3   \n",
      "1           3418339           0                     3   \n",
      "2           3418385           0                     3   \n",
      "3           3418395           0                     3   \n",
      "4           3418442           0                     3   \n",
      "...             ...         ...                   ...   \n",
      "7140019  6439427936           1                     1   \n",
      "7140020  6439432155           0                     0   \n",
      "7140021  6439432155           1                     0   \n",
      "7140022  6439432968           0                     0   \n",
      "7140023  6439432968           1                     0   \n",
      "\n",
      "                                             job name  task index  \\\n",
      "0        ORioZ5deSIAxIuzqmXo1Ivac+22YZbCucJEC0EqRbDc=        19.0   \n",
      "1        ORioZ5deSIAxIuzqmXo1Ivac+22YZbCucJEC0EqRbDc=        13.0   \n",
      "2        k+zdUudIbBQDDLITJLDOG6tynb7k+wBVZgnHyiI2+WU=         0.0   \n",
      "3        Ru4vUIpcJzjdWM1k7VnE3DTNRlbf7v26fiMoQlSSQN4=         0.0   \n",
      "4        agg+iJS8cS2LkWyWggO9QYlkA6jbzCoqnOMbmTUk+G0=         7.0   \n",
      "...                                               ...         ...   \n",
      "7140019  5en4+I5Q/45EwIE5yQ5T1lt4pFfv5C4d1bKbtElpyFg=         0.0   \n",
      "7140020  aPxb6dFdH8wZ2FTBCDuRFDFmTkCuuAJHXUSj1woB7C0=         0.0   \n",
      "7140021  aPxb6dFdH8wZ2FTBCDuRFDFmTkCuuAJHXUSj1woB7C0=         0.0   \n",
      "7140022  Qc2rjkP3kVLHtmurJuvOEwObo2BNkjGGgQhG8gpsS1Y=         0.0   \n",
      "7140023  Qc2rjkP3kVLHtmurJuvOEwObo2BNkjGGgQhG8gpsS1Y=         0.0   \n",
      "\n",
      "         task scheduling class  task CPU request  task memory request  \\\n",
      "0                          3.0           0.18750             0.098390   \n",
      "1                          3.0           0.18750             0.098390   \n",
      "2                          3.0           0.12500             0.086910   \n",
      "3                          3.0           0.12500             0.090820   \n",
      "4                          3.0           0.18750             0.015900   \n",
      "...                        ...               ...                  ...   \n",
      "7140019                    1.0           0.03125             0.011250   \n",
      "7140020                    0.0           0.00000             0.000155   \n",
      "7140021                    0.0           0.00000             0.000155   \n",
      "7140022                    0.0           0.00000             0.000155   \n",
      "7140023                    0.0           0.00000             0.000155   \n",
      "\n",
      "         task disk space request  \n",
      "0                       0.000861  \n",
      "1                       0.000861  \n",
      "2                       0.000470  \n",
      "3                       0.000470  \n",
      "4                       0.000071  \n",
      "...                          ...  \n",
      "7140019                 0.000004  \n",
      "7140020                 0.000000  \n",
      "7140021                 0.000000  \n",
      "7140022                 0.000000  \n",
      "7140023                 0.000000  \n",
      "\n",
      "[7140024 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "task_events_9modified = pd.read_csv (r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\task_events\\task_events_9modified.csv',header=None)\n",
    "task_events_9modified.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request'] \n",
    "merge_9 = pd.merge(job_events_type01modified, task_events_9modified, on = ['job ID', 'event type'])\n",
    "merge_9 = merge_9.dropna()\n",
    "print (merge_9)\n",
    "merge_9.to_csv(\"merge_9.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             job ID  event type  job scheduling class  \\\n",
      "0           3418314           0                     3   \n",
      "1           3418339           0                     3   \n",
      "2           3418356           0                     3   \n",
      "3           3418368           0                     3   \n",
      "4           3418368           0                     3   \n",
      "...             ...         ...                   ...   \n",
      "8066246  6464950166           1                     1   \n",
      "8066247  6464950166           1                     1   \n",
      "8066248  6464950166           1                     1   \n",
      "8066249  6464950166           1                     1   \n",
      "8066250  6464950166           1                     1   \n",
      "\n",
      "                                             job name  task index  \\\n",
      "0        L52XDyhi9x9ChmVBZ1qavOFmnzPeVsvQ2QyGmBZcV4s=         1.0   \n",
      "1        ORioZ5deSIAxIuzqmXo1Ivac+22YZbCucJEC0EqRbDc=         7.0   \n",
      "2        QGf5Bi+7GCnOYjucoJ9xKNPFf9bWGxoEA+M1A1JPvbQ=         4.0   \n",
      "3        mIF6r5X6TTau4MPuBTE+QevbEjjACcfyVeRTAU79lpg=         1.0   \n",
      "4        mIF6r5X6TTau4MPuBTE+QevbEjjACcfyVeRTAU79lpg=         1.0   \n",
      "...                                               ...         ...   \n",
      "8066246  MPfQOOb4nktKkqrBDRCa/n9TJlBxHvscXrV7m6W2Ydo=       184.0   \n",
      "8066247  MPfQOOb4nktKkqrBDRCa/n9TJlBxHvscXrV7m6W2Ydo=       185.0   \n",
      "8066248  MPfQOOb4nktKkqrBDRCa/n9TJlBxHvscXrV7m6W2Ydo=       186.0   \n",
      "8066249  MPfQOOb4nktKkqrBDRCa/n9TJlBxHvscXrV7m6W2Ydo=       187.0   \n",
      "8066250  MPfQOOb4nktKkqrBDRCa/n9TJlBxHvscXrV7m6W2Ydo=       188.0   \n",
      "\n",
      "         task scheduling class  task CPU request  task memory request  \\\n",
      "0                          3.0           0.06250              0.09473   \n",
      "1                          3.0           0.18750              0.09839   \n",
      "2                          3.0           0.18750              0.08948   \n",
      "3                          3.0           0.12500              0.09021   \n",
      "4                          3.0           0.12500              0.09021   \n",
      "...                        ...               ...                  ...   \n",
      "8066246                    1.0           0.06873              0.01910   \n",
      "8066247                    1.0           0.06873              0.01910   \n",
      "8066248                    1.0           0.06873              0.01910   \n",
      "8066249                    1.0           0.06873              0.01910   \n",
      "8066250                    1.0           0.06873              0.01910   \n",
      "\n",
      "         task disk space request  \n",
      "0                       0.000470  \n",
      "1                       0.000861  \n",
      "2                       0.000829  \n",
      "3                       0.000821  \n",
      "4                       0.000821  \n",
      "...                          ...  \n",
      "8066246                 0.000463  \n",
      "8066247                 0.000463  \n",
      "8066248                 0.000463  \n",
      "8066249                 0.000463  \n",
      "8066250                 0.000463  \n",
      "\n",
      "[8066251 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "task_events_10modified = pd.read_csv (r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\task_events\\task_events_10modified.csv',header=None)\n",
    "task_events_10modified.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request'] \n",
    "merge_10 = pd.merge(job_events_type01modified, task_events_10modified, on = ['job ID', 'event type'])\n",
    "merge_10 = merge_10.dropna()\n",
    "print (merge_10)\n",
    "merge_10.to_csv(\"merge_10.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             job ID  event type  job scheduling class  \\\n",
      "0           3418329           0                     3   \n",
      "1           3418339           0                     3   \n",
      "2           3418380           0                     3   \n",
      "3           3418390           0                     3   \n",
      "4           3418405           0                     3   \n",
      "...             ...         ...                   ...   \n",
      "6958719  6486638079           1                     1   \n",
      "6958720  6486638079           1                     1   \n",
      "6958721  6486638079           1                     1   \n",
      "6958722  6486641236           0                     0   \n",
      "6958723  6486641236           1                     0   \n",
      "\n",
      "                                             job name  task index  \\\n",
      "0        EeK3DUWYi1P0vgBTp7wZdUos8UKj/+/FqudTLohMQ9M=         2.0   \n",
      "1        ORioZ5deSIAxIuzqmXo1Ivac+22YZbCucJEC0EqRbDc=        14.0   \n",
      "2        Pg+pSppriyZ7NBvUna6hRjiTnWxykaejs8G943VGjns=         0.0   \n",
      "3        LsgmVCh2BpMrFrIsSDs+J7V3m0Cszc1/3zo9pfMhIC0=         0.0   \n",
      "4        BYQh99FwV+DE3W9a1npuYBtyFK5iTHZpTH6vuu6C+w8=         0.0   \n",
      "...                                               ...         ...   \n",
      "6958719  9XfxgqyEqcJEDRFyjlU9D0jaPwhgk3S02Eb/ZyCdD3M=       186.0   \n",
      "6958720  9XfxgqyEqcJEDRFyjlU9D0jaPwhgk3S02Eb/ZyCdD3M=       187.0   \n",
      "6958721  9XfxgqyEqcJEDRFyjlU9D0jaPwhgk3S02Eb/ZyCdD3M=       188.0   \n",
      "6958722  6cEyQ7YEIiUwugpf7HOg87W3QG7RZOrfzsyOwZwMszw=         0.0   \n",
      "6958723  6cEyQ7YEIiUwugpf7HOg87W3QG7RZOrfzsyOwZwMszw=         0.0   \n",
      "\n",
      "         task scheduling class  task CPU request  task memory request  \\\n",
      "0                          3.0           0.12500             0.099240   \n",
      "1                          3.0           0.18750             0.098390   \n",
      "2                          3.0           0.12500             0.088260   \n",
      "3                          3.0           0.06250             0.122400   \n",
      "4                          3.0           0.18750             0.091060   \n",
      "...                        ...               ...                  ...   \n",
      "6958719                    1.0           0.06873             0.019100   \n",
      "6958720                    1.0           0.06873             0.019100   \n",
      "6958721                    1.0           0.06873             0.019100   \n",
      "6958722                    0.0           0.00000             0.000155   \n",
      "6958723                    0.0           0.00000             0.000155   \n",
      "\n",
      "         task disk space request  \n",
      "0                       0.001049  \n",
      "1                       0.000861  \n",
      "2                       0.000813  \n",
      "3                       0.000594  \n",
      "4                       0.000829  \n",
      "...                          ...  \n",
      "6958719                 0.000463  \n",
      "6958720                 0.000463  \n",
      "6958721                 0.000463  \n",
      "6958722                 0.000000  \n",
      "6958723                 0.000000  \n",
      "\n",
      "[6958724 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "task_events_11modified = pd.read_csv (r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\task_events\\task_events_11modified.csv',header=None)\n",
    "task_events_11modified.columns = ['job ID', 'task index', 'event type', 'task scheduling class', 'task CPU request', 'task memory request', 'task disk space request'] \n",
    "merge_11 = pd.merge(job_events_type01modified, task_events_11modified, on = ['job ID', 'event type'])\n",
    "merge_11 = merge_11.dropna()\n",
    "print (merge_11)\n",
    "merge_11.to_csv(\"merge_11.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_1 = pd.read_csv (r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\merge_1.csv',header=None, skiprows=1)\n",
    "merge_1.columns = ['job ID', 'event type','job scheduling class', 'job name','task index','task scheduling class','task CPU request','task memory request','task disk space request'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_usage_1 = pd.read_csv (r'C:\\Users\\Qinghua\\Desktop\\2\\111AAAProject\\task_usage\\task_usage_1.csv',header=None, skiprows=1,usecols = [1,2,3,4])\n",
    "task_usage_1.columns = ['start time', 'end time', 'job ID', 'task index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.54 GiB for an array with shape (609777653,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d7e30451533e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmerge1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask_usage_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'job ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'task index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\qinghua\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     )\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qinghua\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indicator_pre_merge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m         \u001b[0mjoin_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_join_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         llabels, rlabels = _items_overlap_with_suffix(\n",
      "\u001b[1;32mc:\\users\\qinghua\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    882\u001b[0m             )\n\u001b[0;32m    883\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mleft_indexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_indexer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_join_indexers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qinghua\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_join_indexers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    861\u001b[0m         \u001b[1;34m\"\"\" return the join indexers \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m         return _get_join_indexers(\n\u001b[1;32m--> 863\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    864\u001b[0m         )\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qinghua\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_join_indexers\u001b[1;34m(left_keys, right_keys, sort, how, **kwargs)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     }[how]\n\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mjoin_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\join.pyx\u001b[0m in \u001b[0;36mpandas._libs.join.inner_join\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.54 GiB for an array with shape (609777653,) and data type int64"
     ]
    }
   ],
   "source": [
    "merge1 = pd.merge(merge_1, task_usage_1, on = ['job ID', 'task index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
